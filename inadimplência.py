# -*- coding: utf-8 -*-
"""Inadimplência.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vl7tou5r2C6JHNs-FFgips9avcQ4QWe4
"""

import pandas as pd
import requests
import io
import numpy as np
    
# Downloading the csv file from your GitHub account

url_train = "https://raw.githubusercontent.com/dataminerdbm/test_data_scientist/main/treino.csv" # Make sure the url is the raw version of the file on GitHub
url_test = "https://raw.githubusercontent.com/dataminerdbm/test_data_scientist/main/teste.csv"
download_train = requests.get(url_train).content
download_test = requests.get(url_test).content

# Reading the downloaded content and turning it into a pandas dataframe

df_train = pd.read_csv(io.StringIO(download_train.decode('utf-8')))
df_test = pd.read_csv(io.StringIO(download_test.decode('utf-8')))

# Printing out the first 5 rows of the dataframe

#print(df_train.info())
#print(df_train.info())

# Excluding the lines With NA from Salary, Because it can bias the model with salary = 0
df_train = df_train[~df_train['salario_mensal'].isna()]

# Modify type of numero_de_dependentes to int
df_train['numero_de_dependentes'] = df_train['numero_de_dependentes'].astype(int)
# Cleanig the data from duplicates
df_train.drop_duplicates(keep='first', inplace=True) 
#df_train.describe()

# excluding outliers
def exclude_outliers(DataFrame, col_name):
    interval = 2.9*DataFrame[col_name].std()
    mean = DataFrame[col_name].mean()
    m_i = mean + interval 
    DataFrame = DataFrame[DataFrame[col_name] < m_i]
    return DataFrame

outlier_column = ['util_linhas_inseguras', 'idade', 'vezes_passou_de_30_59_dias', 'razao_debito', 'salario_mensal', 'numero_linhas_crdto_aberto',
                   'numero_emprestimos_imobiliarios', 'numero_de_dependentes']

for col in outlier_column:
    df_train = exclude_outliers(df_train, col)

df_train.describe()
#df_train.info()

# this time I needed to do in less columns
remain_column = ['razao_debito','util_linhas_inseguras','salario_mensal','numero_linhas_crdto_aberto']

for col in remain_column:
    df_train = exclude_outliers(df_train, col)

df_train.describe()

# this time I needed to do in less columns
df_train = exclude_outliers(df_train, 'razao_debito')

df_train.describe()

for col in list(df_train.columns):
    df_train.hist(col, bins = 20)
df_train.describe()

df_train[df_train < 1000].hist('salario_mensal', bins = 40)

df_train = df_train[df_train['salario_mensal'] > 200]
df_train.describe()

for i in list(df_train.columns):
    print(df_train[i].name)
    print(df_train[i].unique())

df_train.describe()

df_train = df_train.fillna(0)
df_train

"""The data is clean, so we can move to the model"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, roc_curve, classification_report,\
                            accuracy_score, confusion_matrix, auc

modelo = smf.glm(formula='inadimplente ~ util_linhas_inseguras + idade + vezes_passou_de_30_59_dias + razao_debito + salario_mensal + numero_linhas_crdto_aberto + numero_vezes_passou_90_dias + numero_emprestimos_imobiliarios + numero_de_vezes_que_passou_60_89_dias + numero_de_dependentes', data=df_train,
                family = sm.families.Binomial()).fit()
print(modelo.summary())

# remove the razao_debito because the test showed this variable wasn't relevant
modelo = smf.glm(formula='inadimplente ~ util_linhas_inseguras + idade + vezes_passou_de_30_59_dias + salario_mensal + numero_linhas_crdto_aberto + numero_vezes_passou_90_dias + numero_emprestimos_imobiliarios + numero_de_vezes_que_passou_60_89_dias + numero_de_dependentes', data=df_train,
                family = sm.families.Binomial()).fit()
print(modelo.summary())

print(np.exp(modelo.params[1:]))

(np.exp(modelo.params[1:]) - 1) * 100

# Agora vamos fazer com sklearn para aproveitar as métricas
model = LogisticRegression(penalty='none', solver='newton-cg')
baseline_df = df_train[['inadimplente', 'util_linhas_inseguras', 'idade', 'vezes_passou_de_30_59_dias', 'salario_mensal', 'numero_linhas_crdto_aberto',
                        'numero_vezes_passou_90_dias', 'numero_emprestimos_imobiliarios', 'numero_de_vezes_que_passou_60_89_dias', 
                        'numero_de_dependentes']].dropna()
y = baseline_df.inadimplente
X = pd.get_dummies(baseline_df[['util_linhas_inseguras', 'idade', 'vezes_passou_de_30_59_dias', 'salario_mensal', 'numero_linhas_crdto_aberto',
                        'numero_vezes_passou_90_dias', 'numero_emprestimos_imobiliarios', 'numero_de_vezes_que_passou_60_89_dias', 
                        'numero_de_dependentes']], drop_first=True)
print(X)

model.fit(X, y)

print(model.coef_)

# Predizendo as probabilidades
yhat = model.predict_proba(X)

yhat = yhat[:, 1] # manter somente para a classe positiva

confusion_matrix(y, model.predict(X)) # usando a função do sklearn

acuracia = accuracy_score(y, model.predict(X))
print('O modelo obteve %0.4f de acurácia.' % acuracia)

print(classification_report(y, model.predict(X)))

print('AUC: %0.2f' % roc_auc_score(y, yhat))

def plot_roc_curve(y_true, y_score, figsize=(10,6)):
    fpr, tpr, _ = roc_curve(y_true, y_score)
    plt.figure(figsize=figsize)
    auc_value = roc_auc_score(y_true, y_score)
    plt.plot(fpr, tpr, color='orange', label='ROC curve (area = %0.2f)' % auc_value)
    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend()
    plt.show()

plot_roc_curve(y, yhat)

df_test = df_test[~df_test['salario_mensal'].isna()]
df_test = df_test[df_test.columns[~df_test.columns.isin(['razao_debito'])]]
df_test.info()

# Modify type of numero_de_dependentes to int
df_train['util_linhas_inseguras'] = df_train['util_linhas_inseguras'].astype(int)
# Cleanig the data from duplicates
df_train.drop_duplicates(keep='first', inplace=True) 
#df_train.describe()

prob = model.predict_proba(df_test)
print(prob)